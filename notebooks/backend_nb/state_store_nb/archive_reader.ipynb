{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7d4eb9",
   "metadata": {},
   "source": [
    "##### Archive reader:\n",
    "\n",
    "When data is requested, this functions to go find the requested data files and return their contents as a clean, sorted list. \n",
    "\n",
    "It's basically the bridge between the raw data files and the rest of the backend, it's the only thing that ever touches the archive folder. It knows how to interpret the data folder and file structure\n",
    "\n",
    "It uses the index.json file from each subject-module-marker combo to quickly filter the files and only open the ones it actually needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91d01d",
   "metadata": {},
   "source": [
    "The logger sets up a logging channel for this specific file. \"__name__\" automatically becomes the path (\"backend.core.state_store.archive_reader\") so that any errors that come from here can be quickly identified. (output prefixed by __name__ value)\n",
    "\n",
    "The function signature defines what data types the dunction expects to receive (strings and datetimes) and return (a list of dicts, one per data point).\n",
    "These are the steps of the read_timeseries() function:\n",
    "\n",
    "    1. **marker_folder:** builds the file path pointing to the folder containing the correct biomarkers by taking all of the folder names as arguments\n",
    "\n",
    "    2. **index_path:** builds the file path pointing to the correct index.json file by taking marker_folder and 'index.json' as arguments. If there it can't find it, ERROR MESSAGE\n",
    "\n",
    "    3. **index:** opens the index.json that it found and converts it into a python dict, just like in load_registry(). \n",
    "\n",
    "    4. **entries:** the entries variable from the index.json is the list of datapoints, each with a value and a timestamp. This function extracts them into a variable.\n",
    "\n",
    "    5. **filtered_entries:** loops through all entries in index, converting all of the timestape strings into datetime objects that it can use. The \"replace(\"Z\", \"+00:00\")\" part is because old python versions don't understand that the Z means UTC, so that it's always understood. After this it checks if the timedate falls within the user-requested window, and if yes it gets added to filtered_entries.\n",
    "\n",
    "    6. **datapoints:** for every filtered entry: the function builds the file path and opens the original raw JSON for each one. If any files are missing or corrupt, it just skips them instead of crashing. \"datapoints\" is a newly created dict that will have the relevant info added to it soon.\n",
    "\n",
    "    7. **parsed_timestamp:** converts the timestamp to a datetime object so that the computer can do math on it.\n",
    "\n",
    "    8. **datapoints.append(datapoint):** populates the datapoints dict with the updated entries that were filtered and have datetime objects as timestamps\n",
    "\n",
    "    9. **sort:** sorts the final list chronologically to make sure that they're in order \n",
    "\n",
    "    10. **return:** returns the final sorted list of filtered datapoints with the right datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_timeseries(\n",
    "    archive_root: str,\n",
    "    subject_id: str,\n",
    "    module_id: str,\n",
    "    marker_id: str,\n",
    "    from_time: datetime,\n",
    "    to_time: datetime,\n",
    ") -> list[dict]:\n",
    "    \n",
    "    marker_folder = os.path.join(archive_root, subject_id, module_id, marker_id)\n",
    "    index_path = os.path.join(marker_folder, \"index.json\")\n",
    "\n",
    "    if not os.path.exists(index_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"No index.json found at {index_path}. \"\n",
    "            f\"Check that subject_id='{subject_id}', module_id='{module_id}', \"\n",
    "            f\"and marker_id='{marker_id}' are correct and that data exists in the archive.\"\n",
    "        )\n",
    "\n",
    "    with open(index_path, encoding=\"utf-8\") as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    entries = index[\"entries\"]\n",
    "\n",
    "    filtered_entries = []\n",
    "    for entry in entries:\n",
    "\n",
    "        entry_time = datetime.fromisoformat(\n",
    "            entry[\"timestamp\"].replace(\"Z\", \"+00:00\")\n",
    "        )\n",
    "\n",
    "        if from_time <= entry_time <= to_time:\n",
    "            filtered_entries.append(entry)\n",
    "\n",
    "    datapoints = []\n",
    "    for entry in filtered_entries:\n",
    "        file_path = os.path.join(marker_folder, entry[\"file\"])\n",
    "\n",
    "        try:\n",
    "            with open(file_path, encoding=\"utf-8\") as f:\n",
    "                datapoint = json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            logger.warning(\n",
    "                \"Skipping data point file '%s': %s\", file_path, e\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        datapoint[\"parsed_timestamp\"] = datetime.fromisoformat(\n",
    "            datapoint[\"timestamp\"].replace(\"Z\", \"+00:00\")\n",
    "        )\n",
    "\n",
    "        datapoints.append(datapoint)\n",
    "\n",
    "    datapoints.sort(key=lambda dp: dp[\"parsed_timestamp\"])\n",
    "\n",
    "    return datapoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asHDT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
